{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "orig_working_directory = os.getcwd()\n",
    "print(orig_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get back up 2 level to be on Desktop first, then navigate to 'gwu-' directory which contains config.py inside\n",
    "os.chdir(os.path.join('..','..','gwu-arl-data-pt-03-2020-u-c'))\n",
    "\n",
    "# Now, you can see the new working directory\n",
    "curr_working_directory = os.getcwd()\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API Keys\n",
    "from config import (consumer_key, \n",
    "                    consumer_secret, \n",
    "                    access_token, \n",
    "                    access_token_secret)\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True) # This'll make the rest of the code obey the rate limit. StackOverFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Search Term ----- BBC, CBS, CNN, Fox, and New York times\n",
    "target_terms = (\"@BBC\", \"@CBS\", \"@CNN\", \"@Fox\", \"@New York times\")\n",
    "\n",
    "# Appended Lists\n",
    "tweets_list = []\n",
    "search_term_list = []\n",
    "\n",
    "# Loop thru all target users\n",
    "for target in target_terms:\n",
    "    \n",
    "    # Iterate thru the ---most recent 100 tweets on target users---\n",
    "     for tweet in tweepy.Cursor(api.search, target, tweet_mode='extended').items(100):\n",
    "        tweets_list.append(tweet)    \n",
    "        search_term_list.append(target)\n",
    "\n",
    "        #tweets_list[0]\n",
    "\n",
    "        user_list = []\n",
    "        text_list = []\n",
    "        createdOn_list = []\n",
    "\n",
    "        compound_list = []\n",
    "        positive_list = []\n",
    "        negative_list = []\n",
    "        neutral_list = []\n",
    "\n",
    "        # Loop through all tweets\n",
    "        for tweet in tweets_list:\n",
    "                    \n",
    "            # Run VADER Analysis on each tweet\n",
    "            tweet_user = tweet.user.screen_name\n",
    "            tweet_text = tweet.full_text\n",
    "            tweet_created = tweet.created_at      \n",
    "        \n",
    "            # Run sentiments analysis using --tweet.full_text--\n",
    "            results = analyzer.polarity_scores(tweet_text)\n",
    "            com = results[\"compound\"]\n",
    "            pos = results[\"pos\"]\n",
    "            neg = results[\"neg\"]\n",
    "            neu = results[\"neu\"]\n",
    "\n",
    "            # Store each value to the appropriate list created above\n",
    "            user_list.append(tweet_user)\n",
    "            text_list.append(tweet_text)\n",
    "            createdOn_list.append(tweet_created)\n",
    "        \n",
    "            compound_list.append(com)\n",
    "            positive_list.append(pos)\n",
    "            negative_list.append(neg)\n",
    "            neutral_list.append(neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'User': user_list,\n",
    "    'Search Term': search_term_list,\n",
    "    'Tweet Text': text_list,\n",
    "    'Created on': createdOn_list,\n",
    "    'Compound': compound_list,\n",
    "    'Positive': positive_list,\n",
    "    'Negative': negative_list,\n",
    "    'Neutral': neutral_list,\n",
    "})\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_sorted = result_df.sort_values(by=['Created on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['Timestamps'] = result_df['Created on'].values.astype(np.int) // 10 ** 9\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('../sentiment_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_max = result_df['Timestamps'].max()\n",
    "#ts_min = result_df['Timestamps'].min()\n",
    "#print(ts_max, ts_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter each News channel to work on ---@BBC, @CBS, @CNN, @Fox, @New York times---\n",
    "bbc = result_df.loc[result_df['Search Term']=='@BBC', :]\n",
    "cbs = result_df.loc[result_df['Search Term']=='@CBS', :]\n",
    "cnn = result_df.loc[result_df['Search Term']=='@CNN', :]\n",
    "fox = result_df.loc[result_df['Search Term']=='@Fox', :]\n",
    "nyt = result_df.loc[result_df['Search Term']=='@New York times', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DATE&TIME to Timestamps\n",
    "#bbc['bbc_ts'] = bbc['Created on'].values.astype(np.int) // 10 ** 9\n",
    "#cbs['cbs_ts'] = cbs['Created on'].values.astype(np.int) // 10 ** 9\n",
    "#cnn['cnn_ts'] = cnn['Created on'].values.astype(np.int) // 10 ** 9\n",
    "#fox['fox_ts'] = fox['Created on'].values.astype(np.int) // 10 ** 9\n",
    "#nyt['nyt_ts'] = nyt['Created on'].values.astype(np.int) // 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.scatter(bbc['Timestamps'], bbc['Compound'], color=\"hotpink\", alpha=0.8)\n",
    "ax.scatter(cbs['Timestamps'], cbs['Compound'], color=\"gold\", alpha=0.8)\n",
    "ax.scatter(cnn['Timestamps'], cnn['Compound'], color=\"seagreen\", alpha=0.8)\n",
    "ax.scatter(fox['Timestamps'], fox['Compound'], color=\"royalblue\", alpha=0.8)\n",
    "ax.scatter(nyt['Timestamps'], nyt['Compound'], color=\"mediumvioletred\", alpha=0.8)\n",
    "\n",
    "ax.set_ylim(-1, 1)\n",
    "#ax.set_xlim(100, 0)\n",
    "ax.set_xlabel('News Channels', fontsize=13)\n",
    "ax.set_ylabel('Scores', fontsize=13)\n",
    "ax.set_title('Sentiment Intensity Score for each News Organizations', fontsize=15)\n",
    "\n",
    "ax.grid(alpha=.2)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_av = bbc['Compound'].mean()\n",
    "cbs_av = cbs['Compound'].mean()\n",
    "cnn_av = cnn['Compound'].mean()\n",
    "fox_av = fox['Compound'].mean()\n",
    "nyt_av = nyt['Compound'].mean()\n",
    "\n",
    "index = [\"@BBC\", \"@CBS\", \"@CNN\", \"@Fox\", \"@New York times\"]\n",
    "\n",
    "df = pd.DataFrame({'Channels': ['BBC', 'CBS', 'CNN', 'FOX', 'NYT'], \n",
    "                   'Tweet Polarity': [bbc_av, cbs_av, cnn_av, fox_av, nyt_av]})\n",
    "\n",
    "ax = df.plot.bar(x='Channels', y='Tweet Polarity', rot=0)\n",
    "\n",
    "ax.set_xlabel('News Channels', fontsize=13)\n",
    "ax.set_ylabel('Scores', fontsize=13)\n",
    "ax.set_title('Overall Media Sentiment based on Twitter (May 22nd, 2020)', fontsize=14)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.set_ylim(-0.1, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
